{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468ba8a4",
   "metadata": {},
   "source": [
    "# Assignment A2: Topic Modeling and Text ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e8e86",
   "metadata": {},
   "source": [
    "Covering material from Notebooks 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230fa4a-f7b7-4660-83da-12560e634321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()\n",
    "\n",
    "\n",
    "import spacy\n",
    "dfs = df.sample(200)\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c537e-1574-4b60-876f-ca99f4ccfdb9",
   "metadata": {},
   "source": [
    "# A. Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f3892-4da7-48a5-a3cb-c0f5b0a2d54a",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d8db7-598b-4959-a356-6ac677d44509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,svd_solver='randomized')\n",
    "\n",
    "##TODO reduce the vectorized data using PCA\n",
    "##TODO compute again cosine similarity with the reduced version for the first 200 snippets\n",
    "##TODO for the first snippet, show again its three most similar snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f925f-35c6-4e9a-8877-40b9897dd888",
   "metadata": {},
   "source": [
    "Compare the cosine similarity between docs before and after PCA reduction. Did the results change? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f94fca-4df9-4e47-9b80-5f25ace34ab7",
   "metadata": {},
   "source": [
    "## Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981d9ac",
   "metadata": {},
   "source": [
    "For this part you will need to use LDA Mallet. If you cannot have Mallet run, you can use the simple LDA algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d619b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "##TODO create a dictionary with the pre-processed tokenized text and filter it according to frequencies and keeping 1000 vocabularies\n",
    "##TODO create the doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ee87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO train a LDA Mallet model with 5, 10 and 15 topics\n",
    "##TODO compute the coherence score for each of these model and print the topics from the model with highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "##TODO using LDAvis visualize the topics using the optimal number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeea966-5d59-437d-8d8d-bf733491f8c6",
   "metadata": {},
   "source": [
    "# B, Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b5a6b-a8a8-40ed-8c6c-8fc09f9d1b12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load and Pre-process Text\n",
    "We do sentiment analysis on the [Movie Review Data](https://www.cs.cornell.edu/people/pabo/movie-review-data/). If you would like to know more about the data, have a look at [the paper](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf) (but no need to do so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd79c5-62bf-41ce-8ae5-a9d06d324d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we do sentiment analysis\n",
    "# download the data\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar xf aclImdb_v1.tar.gz\n",
    "\n",
    "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
    "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
    " \n",
    "!tar xf scale_data.tar.gz \n",
    "!tar xf scale_whole_review.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6cec7-8c59-47d5-b5e6-a337d7402a39",
   "metadata": {},
   "source": [
    "First, we have to load the data for which we provide the function below. Note how we also preprocess the text using gensim's simple_preprocess() function and how we already split the data into a train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08375a-7437-46e0-983b-f821285ebdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "def load_data():\n",
    "    examples, labels = [], []\n",
    "    authors = os.listdir(\"scale_whole_review\")\n",
    "    for author in authors:\n",
    "        path = os.listdir(os.path.join(\"scale_whole_review\", author, \"txt.parag\"))\n",
    "        fn_ids = os.path.join(\"scaledata\", author, \"id.\" + author)\n",
    "        fn_ratings = os.path.join(\"scaledata\", author, \"rating.\" + author)\n",
    "        with open(fn_ids) as ids, open(fn_ratings) as ratings:\n",
    "            for idx, rating in zip(ids, ratings):\n",
    "                labels.append(float(rating.strip()))\n",
    "                filename_text = os.path.join(\"scale_whole_review\", author, \"txt.parag\", idx.strip() + \".txt\")\n",
    "                with open(filename_text, encoding='latin-1') as f:\n",
    "                    examples.append(\" \".join(simple_preprocess(f.read())))\n",
    "    return examples, labels\n",
    "                  \n",
    "X,y  = load_data()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print (\"text:\", X_train[0], \"\\nlabel:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68000f-c995-4574-9bef-12b5ed97c4ba",
   "metadata": {},
   "source": [
    "## Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1c2a3-4bee-49f6-9cb9-9c314252ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a TF_IDF Vectorizer on X_train and vectorize X_train and X_test\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                        max_df=.5,  \n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "##TODO train vectorizer\n",
    "\n",
    "##TODO transform X_train to TF-IDF values\n",
    "X_train_tfidf = ...\n",
    "##TODO transform X_test to TF-IDF values\n",
    "X_test_tfidf = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee471c2-97d4-4b31-8470-b34604726e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO scale both training and test data with the standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe950e-0ff6-42b7-a43d-8f0250edba1b",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14eef73-0547-48b4-bb83-8c892bb76302",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO train an elastic net on the transformed output of the scaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet(alpha=0.01)\n",
    "\n",
    "##TODO train the ElasticNet\n",
    "\n",
    "##TODO predict the testset\n",
    "\n",
    "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, balanced_accuracy_score\n",
    "##TODO print mean squared error and r2 score on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f412b-2744-4eef-98af-f06578a21b6f",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3e0b9-71ee-41fd-aeca-2bfea32620dd",
   "metadata": {},
   "source": [
    "Next, we train an OLS model doing binary prediction on these movie reviews. Two get two bins, we transform the continuous ratings into two classes, where one class contains all the negative ratings (value < 0.5), the other class all the positive ratings (value > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c406a6-c6fa-4c3f-93c0-cde7ba75ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [1 if i >= 0.5 else 0 for i in y_train]\n",
    "y_test = [1 if i >= 0.5 else 0 for i in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a76fc-542f-4412-91aa-fbdc33548ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO train logistic regression on X_train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "##TODO train a logistic regression\n",
    "\n",
    "##TODO predict the testset \n",
    "\n",
    "##since we have continuous output, we need to post-process our labels into two classes. We choose a threshold of 0.5 \n",
    "def map_predictions(predicted):\n",
    "    predicted = [1 if i > 0.5 else 0 for i in predicted]\n",
    "    return predicted\n",
    "\n",
    "##TODO print the accuracy of our classifier on the testset\n",
    "\n",
    "## TODO print the 10 most informative words of the regression (the 10 words having the highest coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954fedb-c300-44ec-b86f-a19ebed21c18",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2db522-6325-4ffa-9e73-83c876ab11df",
   "metadata": {},
   "source": [
    "Lastly, we train an XGBoost classifier to do topic prediction on the AG news dataset, which is a multi-class prediction problem (4 classes). We again have to vectorize the data, train the classifier, predict the testset and output an evaluation metric (we go for accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67037ba5-a76e-48b3-a3dd-3e24d2654480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84cb76b-7a64-43bb-b23e-1109236401d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf856fd-10c7-44e3-9daa-66ea8d134a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# only consider 10% of the data\n",
    "dfs = df.sample(frac=0.1)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfs[\"text\"], dfs[\"label\"], test_size=0.33, random_state=42)\n",
    "\n",
    "vec = TfidfVectorizer(min_df=5, # at min 1% of docs\n",
    "                        max_df=.5,  \n",
    "                        stop_words='english',\n",
    "                        max_features=2000,\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "# transform into TF-IDF values\n",
    "X_train_tfidf = vec.fit_transform(X_train).todense()\n",
    "X_test_tfidf = vec.transform(X_test).todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cca893-663b-4977-a590-ebe679717e28",
   "metadata": {},
   "source": [
    "XGBoost provides an interface to SKLearn classifiers, e.g. they implement the same train and predict methods as an SKLearn classifier would. If you are interested in a more detailed overview, have a look at the [official documentation](https://xgboost.readthedocs.io/en/latest/python/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e6c85-253c-41ae-aaa5-c64e325287e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'objective':'multi:softmax', 'num_class': 5, 'n_estimators':25}\n",
    "# note how we only have 4 labels, but we need to pass \"num_class\": 5\n",
    "# if we pass \"num_class\": 4, we get the error \"label must be in [0, num_class).\"\n",
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBModel(**param_dist)\n",
    "\n",
    "##TODO train the XGBModel \n",
    "\n",
    "##TODO predict the testset \n",
    "\n",
    "##TODO evaluate the predictions using accuracy as a metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
