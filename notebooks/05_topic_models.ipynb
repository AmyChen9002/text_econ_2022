{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# Lecture 5. Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing for Law and Social Science<br>\n",
    "Elliott Ash, ETH Zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:40:51.192954Z",
     "start_time": "2022-03-11T09:40:51.186493Z"
    }
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import numpy as np\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:40:51.740668Z",
     "start_time": "2022-03-11T09:40:51.614198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('sc_cases_cleaned.pkl',compression='gzip')\n",
    "X = pd.read_pickle('X.pkl').toarray()\n",
    "X_tfidf = pd.read_pickle('X_tfidf.pkl').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T09:40:52.236650Z",
     "start_time": "2022-03-11T09:40:52.209792Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "text0 = ' '.join(simple_preprocess(df['opinion_text'][0]))\n",
    "text1 = ' '.join(simple_preprocess(df['opinion_text'][1]))\n",
    "\n",
    "text1[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.548510Z",
     "start_time": "2022-03-11T07:07:20.395612Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Principal Components\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,svd_solver='randomized')\n",
    "Xpca = pca.fit_transform(X)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.624942Z",
     "start_time": "2022-03-11T07:07:20.563819Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PCA Viz\n",
    "plt.scatter(Xpca[:,0],Xpca[:,1], alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:20.771563Z",
     "start_time": "2022-03-11T07:07:20.636987Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PCA 3D Viz\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "Axes3D(plt.figure()).scatter(Xpca[:,0],Xpca[:,1], Xpca[:,2], alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:21.644782Z",
     "start_time": "2022-03-11T07:07:20.780568Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% make components to explain 95% of variance\n",
    "pca = PCA(n_components=.95)\n",
    "X95 = pca.fit_transform(X)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:21.792302Z",
     "start_time": "2022-03-11T07:07:21.645487Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PCA Inverse Transform\n",
    "Xrestore = pca.inverse_transform(X95)\n",
    "plt.plot(Xrestore[0],X[0],'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:32.071529Z",
     "start_time": "2022-03-11T07:07:21.816551Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Incremental PCA\n",
    "X_mm = np.memmap('X.pkl',shape=(32567, 525))\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "inc_pca = IncrementalPCA(n_components=100, batch_size=1000)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:32.079855Z",
     "start_time": "2022-03-11T07:07:32.072452Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PC Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "Y = df['log_cite_count']\n",
    "lin_reg = LinearRegression()\n",
    "scores = cross_val_score(lin_reg,\n",
    "                         X95[:,:10],\n",
    "                         Y) \n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:36.448807Z",
     "start_time": "2022-03-11T07:07:32.082221Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% MDS, Isomap, and T-SNE\n",
    "from sklearn.manifold import MDS, Isomap, TSNE\n",
    "mds = MDS(n_components=2)\n",
    "Xmds = mds.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xmds[:,0],Xmds[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:36.765819Z",
     "start_time": "2022-03-11T07:07:36.472825Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "Xiso = iso.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xiso[:,0],Xiso[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:07:38.158032Z",
     "start_time": "2022-03-11T07:07:36.781207Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% t-SNE\n",
    "tsne = TSNE(n_components=2, n_iter=250)\n",
    "Xtsne = tsne.fit_transform(X[:500,:200])\n",
    "Axes3D(plt.figure()).scatter(Xtsne[:,0],Xtsne[:,1], alpha=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "For further reference see the material from topic [modeling with gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T08:04:52.181598Z",
     "start_time": "2022-03-11T08:02:37.752474Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean document\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from tqdm import tqdm as tq\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# this is faster and we don't need the whole grammatical parse analysis\n",
    "\n",
    "def tokenize(x, nlp):\n",
    "    # lemmatize and lowercase without stopwords, punctuation and numbers\n",
    "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit and len(w) > 2]\n",
    "\n",
    "# split into paragraphs\n",
    "doc_clean = []\n",
    "for doc in tq(df['opinion_text'][:100]):\n",
    "    # split by paragraph\n",
    "    for paragraph in doc.split(\"\\n\\n\"):\n",
    "        doc_clean.append(tokenize(paragraph, nlp))\n",
    "print (doc_clean[:10])\n",
    "\n",
    "\n",
    "# randomize document order\n",
    "from random import shuffle\n",
    "shuffle(doc_clean)\n",
    "\n",
    "# creating the term dictionary\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "# filter extremes, drop all words appearing in less than 10 paragraphs and all words appearing in at least every third paragraph\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.33, keep_n=1000)\n",
    "print (len(dictionary))\n",
    "\n",
    "\n",
    "# creating the document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# train LDA with 10 topics and print\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "lda = LdaModel(doc_term_matrix, num_topics=10, \n",
    "               id2word = dictionary, passes=3)\n",
    "lda.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-11T10:01:55.267Z"
    }
   },
   "outputs": [],
   "source": [
    "# to get the topic proportions for a document, use\n",
    "# the corresponding row from the document-term matrix.\n",
    "lda[doc_term_matrix[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-11T10:01:49.081Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# or, for all documents\n",
    "[lda[d] for d in doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:26:40.906548Z",
     "start_time": "2022-03-11T07:26:36.749256Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# LDA Word Clouds\n",
    "###\n",
    "\n",
    "from numpy.random import randint\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make directory if not exists\n",
    "from os import mkdir\n",
    "try:\n",
    "    mkdir('lda')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# make word clouds for the topics\n",
    "for i,weights in lda.show_topics(num_topics=-1,\n",
    "                                 num_words=100,\n",
    "                                 formatted=False):\n",
    "    \n",
    "    #logweights = [w[0], np.log(w[1]) for w in weights]\n",
    "    maincol = randint(0,360)\n",
    "    def colorfunc(word=None, font_size=None, \n",
    "                  position=None, orientation=None, \n",
    "                  font_path=None, random_state=None):   \n",
    "        color = randint(maincol-10, maincol+10)\n",
    "        if color < 0:\n",
    "            color = 360 + color\n",
    "        return \"hsl(%d, %d%%, %d%%)\" % (color,randint(65, 75)+font_size / 7, randint(35, 45)-font_size / 10)   \n",
    "\n",
    "    \n",
    "    wordcloud = WordCloud(background_color=\"white\", \n",
    "                          ranks_only=False, \n",
    "                          max_font_size=120,\n",
    "                          color_func=colorfunc,\n",
    "                          height=600,width=800).generate_from_frequencies(dict(weights))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:16:19.464120Z",
     "start_time": "2022-03-11T07:16:17.547251Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyLDAvis, for more details, refer to https://github.com/bmabey/pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Mallet to calculate coherence scores for different number of topics to automatically determine the best number of topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T08:37:27.411606Z",
     "start_time": "2022-03-11T08:31:06.461743Z"
    }
   },
   "outputs": [],
   "source": [
    "# you need gensim version <= 3.8.3 for this to work\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "mallet_path = '~/Downloads/mallet-2.0.8/bin/mallet'\n",
    "scores = []\n",
    "for num_topics in range(2, 20, 2):\n",
    "    print (num_topics)\n",
    "    lda = LdaMallet(mallet_path, doc_term_matrix, num_topics=num_topics, id2word=dictionary)\n",
    "    coherence = CoherenceModel(model=lda, texts=doc_clean, corpus=doc_term_matrix, dictionary=dictionary, coherence='c_v')\n",
    "    scores.append((num_topics, coherence.get_coherence()))\n",
    "pd.DataFrame(scores, columns=[\"Number of Topics\", \"Coherence Scores\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "For further reference for this and the following section see [here](https://github.com/fastai/course-nlp/blob/219d0c217bd83339e21471d31cd787e86d6ec0a0/2-svd-nmf-topic-modeling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:30:57.630094Z",
     "start_time": "2022-03-11T07:30:56.943444Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "X = pd.read_pickle('X.pkl').todense()\n",
    "vec = pd.read_pickle('vec-3grams-1.pkl')\n",
    "vocab = np.array(vec.get_feature_names())\n",
    "vocab[400:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:00.112146Z",
     "start_time": "2022-03-11T07:30:59.201038Z"
    }
   },
   "outputs": [],
   "source": [
    "U, s, Vh = linalg.svd(X, full_matrices=False)\n",
    "print(U.shape, s.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:01.693951Z",
     "start_time": "2022-03-11T07:31:01.399719Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:03.071259Z",
     "start_time": "2022-03-11T07:31:03.057501Z"
    }
   },
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "show_topics(Vh[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization (NMF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:06.882377Z",
     "start_time": "2022-03-11T07:31:04.922245Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "clf = decomposition.NMF(n_components=10, random_state=1)\n",
    "\n",
    "W1 = clf.fit_transform(X)\n",
    "H1 = clf.components_\n",
    "\n",
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:09.147221Z",
     "start_time": "2022-03-11T07:31:09.077828Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import temporary_file\n",
    "\n",
    "df = df.reset_index()\n",
    "df['id'] = df.index\n",
    "author2doc = df[:100][['authorship','id']]\n",
    "author2doc = author2doc.groupby('authorship').apply(lambda x: list(x['id'])).to_dict()\n",
    "author2doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-11T07:31:10.623422Z",
     "start_time": "2022-03-11T07:31:10.554968Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AuthorTopicModel(\n",
    "        doc_term_matrix, author2doc=author2doc, id2word=dictionary, num_topics=10)\n",
    "\n",
    "# For each author list topic distribution\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    "author_vecs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
